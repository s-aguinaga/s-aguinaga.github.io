<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Sal Aguinaga</title>
    <link>http://s-aguinaga.github.io/index.xml</link>
    <description>Recent content on Sal Aguinaga</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Wed, 09 Aug 2017 09:19:15 -0500</lastBuildDate>
    <atom:link href="http://s-aguinaga.github.io/index.xml" rel="self" type="application/rss+xml" />
    
    <item>
      <title>Neural Network Architectures</title>
      <link>http://s-aguinaga.github.io/post/neuralnetarch/</link>
      <pubDate>Wed, 09 Aug 2017 09:19:15 -0500</pubDate>
      
      <guid>http://s-aguinaga.github.io/post/neuralnetarch/</guid>
      <description>&lt;p&gt;This projects aims to examine the complexity of neural network architectures to leverage structure
that improves the training phase.&lt;/p&gt;

&lt;p&gt;&lt;/p&gt;

&lt;h1 id=&#34;notebook&#34;&gt;Notebook&lt;/h1&gt;

&lt;p&gt;My notes on many things ML and DL&lt;/p&gt;

&lt;h2 id=&#34;conferences&#34;&gt;Conferences&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;International Conference on Learning Representations
2017 (April 24 - 26, 2017)`&lt;/li&gt;
&lt;li&gt;AAAI (2 – 7 February  –  New Orleans, Louisiana, USA)&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;argonne-ml&#34;&gt;Argonne ML&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;NN Zoo&lt;/li&gt;
&lt;li&gt;NEURAL ARCHITECTURE SEARCH WITH REINFORCEMENT LEARNING,
Barret Zoph∗, Quoc V. Le (barretzoph, qvl@google.com)&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;network-architectures&#34;&gt;Network Architectures&lt;/h2&gt;

&lt;p&gt;-[] NEURAL ARCHITECTURE SEARCH WITH REINFORCEMENT LEARNING
        &lt;a href=&#34;https://openreview.net/pdf?id=r1Ue8Hcxg&#34;&gt;https://openreview.net/pdf?id=r1Ue8Hcxg&lt;/a&gt;
-   _ &lt;a href=&#34;http://wikicoursenote.com/wiki/Deep_Convolutional_Neural_Networks_For_LVCSR&#34;&gt;Deep Convolutional Neural Networks For LVCSR&lt;/a&gt;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;[] Designing Neural Network Architectures using Reinforcement Learning&lt;/li&gt;
&lt;li&gt;[] Making Neural Programming Architectures Generalize via Recursion&lt;/li&gt;
&lt;li&gt;[] DSD: Dense-Sparse-Dense Training for Deep Neural Networks&lt;/li&gt;

&lt;li&gt;&lt;p&gt;[] Introspection:Accelerating Neural Network Training By Learning Weight Evolution&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;[]&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Incremental Growth of Semantic Branches on CNNs via Multi-Shot Learning Quanshi Zhang, Ruiming Cao, Ying Nian Wu and Song-Chun Zhu&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Unsupervised Large Graph Embedding Feiping Nie, Wei Zhu and Xuelong Li&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Regularization for Unsupervised Deep Neural Nets Baiyang Wang and Diego Klabjan&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Efficient Hyperparameter Optimization of Deep Learning Algorithms Using Deterministic RBF Surrogates
Ilija Ilievski, Jiashi Feng, Taimoor Akhtar and Christine Shoemaker&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Tunable Sensitivity to Large Errors in Neural Network Training Gil Keren, Sivan Sabato and Björn Schuller&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Understanding the Semantic Structures of Tables with a Hybrid Deep Neural Network Architecture
Kyosuke Nishida, Kugatsu Sadamitsu, Ryuichiro Higashinaka and Yoshihiro Matsuo&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;a href=&#34;https://arxiv.org/pdf/1606.04474v1.pdf&#34;&gt;Learning to learn by gradient descent by gradient descent&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Feature engineering - the data you have may have all info that is required by the model, but these
might not be in a mode that can be leveraged.&lt;/p&gt;

&lt;h2 id=&#34;hyperparameters&#34;&gt;Hyperparameters&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;http://fastml.com/tuning-hyperparams-fast-with-hyperband/&#34;&gt;Hyperban by fastml&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://people.eecs.berkeley.edu/~kjamieson/hyperband.html&#34;&gt;Hyperband at Berkeley&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;basic-definitions&#34;&gt;Basic Definitions&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;Affine&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Deep Learning](&lt;a href=&#34;http://neuralnetworksanddeeplearning.com/chap6.html&#34;&gt;http://neuralnetworksanddeeplearning.com/chap6.html&lt;/a&gt;)&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;DL Bells and Wistles:
&lt;a href=&#34;http://colinraffel.com/wiki/neural_network_hyperparameters&#34;&gt;Nerual Network Hyperparameters&lt;/a&gt;
&lt;a href=&#34;http://www.iro.umontreal.ca/~bengioy/papers/YB-tricks.pdf&#34;&gt;Hyper-Parameters&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;It has been shown that the use of computer clusters for hyper-parameter selection can have an important effect on results (Pinto et al., 2009).&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;We define a hyper-parameter for a learning algorithm A as a value to be selected prior to the ac- tual application of A to the data, a value that is not directly selected by the learning algorithm itself.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Matrices:
Hessian matrix&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;a square matrix of  second-order paritla derivatives of a scalar-valued function or scalar-field.&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Gauss-Newton matrix&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Fisher information matrix&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&#34;training-time&#34;&gt;Training Time&lt;/h3&gt;

&lt;p&gt;What is a long time? So in reference to standard deep learning tasks when working with
standard datasets such as MNIST,&lt;/p&gt;

&lt;h3 id=&#34;different-nn-architectures&#34;&gt;Different NN Architectures&lt;/h3&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;On ReLU (rectifying linear unit)&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://stats.stackexchange.com/questions/188083/how-to-know-where-to-put-bias-terms-in-neural-nets&#34;&gt;Dealing with the bias term in NN&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;See Yoshua Bengio and Yann Dauphin&amp;rsquo;s article Big Neural Networks Waste Capacity, which touches on this issue and explores the diminishing returns issue when constructing bigger or deeper neural networks.&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Weigts evolution
This is an interesting concept.&lt;/p&gt;

&lt;h2 id=&#34;other-interesting-papers&#34;&gt;Other Interesting Papers&lt;/h2&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;a href=&#34;https://github.com/carpedm20/simulated-unsupervised-tensorflow&#34;&gt;TensorFlow implementation of &amp;ldquo;Learning from Simulated and Unsupervised Images through Adversarial Training&amp;rdquo;&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;frameworks&#34;&gt;Frameworks&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://www.tensorflow.org/tutorials/deep_cnn&#34;&gt;Convolutional Neural Networks&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&#34;tensorflow&#34;&gt;tensorflow&lt;/h3&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://www.matroid.com/dlwithtf/chap1-2.pdf&#34;&gt;Getting started with tensorflow&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.tensorflow.org/install/install_mac#ValidateYourInstallation&#34;&gt;validate the installation &lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;To avoid the warning, let&amp;rsquo;s install tensorflow from source.
* Dependencies
    - &lt;code&gt;brew install bazel&lt;/code&gt;
    - &lt;a href=&#34;https://docs.bazel.build/versions/master/install-os-x.htmlhttps://docs.bazel.build/versions/master/install-os-x.html&#34;&gt;Install Bazel&lt;/a&gt;
    Once installe, you can upgrade to a newer version of Bazel with:&lt;code&gt;sudo apt-get upgrade bazel&lt;/code&gt;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Warnings

&lt;ul&gt;
&lt;li&gt;W &lt;code&gt;tensorflow/core/platform/cpu_feature_guard.cc:45&lt;/code&gt; The TensorFlow library wasn&amp;rsquo;t compiled to use SSE4.2 instructions&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;Invalid path to CUDA 8.0 toolkit. /usr/local/cuda/lib/libcudart.8.0.dylib cannot be found&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;</description>
    </item>
    
    <item>
      <title>News Archive</title>
      <link>http://s-aguinaga.github.io/post/newsarchive/</link>
      <pubDate>Thu, 22 Jun 2017 09:53:32 -0400</pubDate>
      
      <guid>http://s-aguinaga.github.io/post/newsarchive/</guid>
      <description>&lt;p&gt;Archived news
&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Mar 2017 - Invited talk at Argonne National Lab&lt;br&gt;&lt;/li&gt;
&lt;li&gt;Mar 2017 - Invited Talk at KPMG&lt;br&gt;&lt;/li&gt;
&lt;li&gt;Jan 2017 - Teaching Assistant for &lt;a href=&#34;https://www3.nd.edu/courses//cse/cse40884.01/www/index.html&#34;&gt;Network Science CSE-40884 / 60884&lt;/a&gt;&lt;br&gt;&lt;/li&gt;
&lt;li&gt;Nov 2016 - Paper with Aastha Nigam (she is first author) and Nitesh V. Chawla &lt;a href=&#34;http://besc-conf.org/2016/accepted_papers.html&#34;&gt;BESC&lt;/a&gt;&lt;br&gt;&lt;/li&gt;
&lt;li&gt;Oct 2016 - Presented research paper @ &lt;a href=&#34;http://www.cikm2016.org/&#34;&gt;CIKM2016&lt;/a&gt;&lt;br&gt;&lt;/li&gt;
&lt;li&gt;Sep 2016 - Funding award to attend the &lt;a href=&#34;http://www.heidelberg-laureate-forum.org&#34;&gt;4th Heildelberg Laureate Forum&lt;/a&gt; where I got to meet and talk Nobel Laureates and Turin Award winners and an incredible group of young researchers from across the world.&lt;br&gt;&lt;/li&gt;
&lt;li&gt;Sep 2016 - Attended the &lt;a href=&#34;http://people.cs.uchicago.edu/~aelmore/mbdoc.html&#34;&gt;Midwest Big Data Opportunities and Challenges Workshop&lt;/a&gt;&lt;br&gt;&lt;/li&gt;
&lt;li&gt;Aug 2016 - Presented an accepted paper and poster at &lt;a href=&#34;http://www.mlgworkshop.org/2016&#34;&gt;12th International Workshop on Mining and Learning with Graphs&lt;/a&gt; held in conjuction with KDD16&lt;/li&gt;
&lt;/ul&gt;</description>
    </item>
    
    <item>
      <title>openACC</title>
      <link>http://s-aguinaga.github.io/post/openacc/</link>
      <pubDate>Tue, 06 Dec 2016 12:01:45 -0500</pubDate>
      
      <guid>http://s-aguinaga.github.io/post/openacc/</guid>
      <description>&lt;h2 id=&#34;parallelize-your-code&#34;&gt;Parallelize Your Code&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;Benefits:

&lt;ul&gt;
&lt;li&gt;No involvement of OpenCL, CUDA, etc.&lt;/li&gt;
&lt;li&gt;Source can work on both CPUs (serially) and can be deployed to GPUs&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;/p&gt;

&lt;h2 id=&#34;directives&#34;&gt;Directives&lt;/h2&gt;

&lt;p&gt;The directive way approach has shown that one stands to gain by improving the
performance of code execution.&lt;/p&gt;

&lt;p&gt;Give hints to the compiler so it knows what to do.&lt;/p&gt;

&lt;p&gt;&lt;code&gt;#pragma omp parallel for reduction(+:pi)&lt;/code&gt;
and its equivalent in OpenACC: &lt;code&gt;#pragma acc kernels&lt;/code&gt;
This is something that is usually added right before code that can be highly
parallelizable. Takes serial code and makes run on GPUs.&lt;/p&gt;

&lt;p&gt;&lt;code&gt;#pragma acc kernels&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;Put a directive in front of the loops, then the compiler will generate the right
gpu code.&lt;/p&gt;

&lt;h2 id=&#34;syntax&#34;&gt;Syntax&lt;/h2&gt;

&lt;p&gt;&lt;code&gt;pragma&lt;/code&gt; is a directive of instruction to the compilers.&lt;/p&gt;

&lt;p&gt;&lt;code&gt;#pragma acc kernels [clause ...]&lt;/code&gt;
is followed by structure blocks such as for loop in C.&lt;/p&gt;

&lt;h2 id=&#34;c-pointers&#34;&gt;C Pointers&lt;/h2&gt;

&lt;p&gt;Using keywords like &lt;code&gt;restrict&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&lt;code&gt;float *restrict y&lt;/code&gt; when nothing is going to point to &lt;code&gt;y&lt;/code&gt;, then
use restrict.&lt;/p&gt;

&lt;p&gt;To activate the directives:
&lt;code&gt;pgcc -acc -Minfo=accel saxpy.c&lt;/code&gt;&lt;/p&gt;

&lt;h1 id=&#34;parallel-programming-using-the-multiprocessing-module-in-python&#34;&gt;Parallel programming using the Multiprocessing module in Python&lt;/h1&gt;

&lt;p&gt;Here, I take a look at the &lt;a href=&#34;https://docs.python.org/dev/library/multiprocessing.html&#34;&gt;multiprocessing&lt;/a&gt; module&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Learning from Others</title>
      <link>http://s-aguinaga.github.io/post/learningfromothers/</link>
      <pubDate>Sun, 04 Dec 2016 04:52:33 -0500</pubDate>
      
      <guid>http://s-aguinaga.github.io/post/learningfromothers/</guid>
      <description>&lt;p&gt;Sites I follow. A collection of technical content pages that I admire and learn a ton from.&lt;/p&gt;

&lt;p&gt;&lt;/p&gt;

&lt;h1 id=&#34;links-to-amazing-websites&#34;&gt;Links to Amazing Websites&lt;/h1&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;&lt;a href=&#34;ven://aric.hagberg.org&#34;&gt;Aric Hagberg&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;a href=&#34;https://jeremykun.com/about/&#34;&gt;Jeremy Kun&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;a href=&#34;http://acmsel.safaribooksonline.com/9781491901427&#34;&gt;Data Science from Scratch&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;</description>
    </item>
    
    <item>
      <title>Graph mining and generators</title>
      <link>http://s-aguinaga.github.io/post/graph-mining-generators/</link>
      <pubDate>Sun, 13 Nov 2016 12:00:04 -0500</pubDate>
      
      <guid>http://s-aguinaga.github.io/post/graph-mining-generators/</guid>
      <description>&lt;p&gt;Simple examples on algorithms typically used in graph mining (work in progress).&lt;/p&gt;

&lt;p&gt;&lt;/p&gt;

&lt;h2 id=&#34;introduction&#34;&gt;Introduction&lt;/h2&gt;

&lt;p&gt;This project welcomes open collaboration and feedback.&lt;/p&gt;

&lt;h2 id=&#34;graph-mining&#34;&gt;Graph Mining&lt;/h2&gt;

&lt;!-- www.oslom.org by fortunato --&gt;

&lt;h3 id=&#34;visualization&#34;&gt;Visualization&lt;/h3&gt;

&lt;p&gt;Using &lt;em&gt;tkplot&lt;/em&gt; to visualize a simple graph on macOS isn&amp;rsquo;t the best because I keep
a pop-up window that I don&amp;rsquo;t like one bit.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r visualize, echo=F&#34;&gt;library(intergraph)
library(igraph)
# note that the graphs converted to dimacs format can be read directly like this
g       &amp;lt;- read.graph(&amp;quot;INDDGO/sample_graphs/WikiExample.graph&amp;quot;, format = &amp;quot;dimacs&amp;quot;,directed=F)
Coord   &amp;lt;- tkplot(g, vertex.size=3, vertex.label=V(g)$role,vertex.color=&#39;darkgreen&#39;)
MCoords &amp;lt;- tkplot.getcoords(Coord)
plot(g, layout=MCoords, vertex.size=8, vertex.label=NA, vertex.color=&amp;quot;lightblue&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;marginalization-in-general-graphs&#34;&gt;Marginalization in general graphs&lt;/h3&gt;

&lt;p&gt;The marginal distribution of a subset of a collection of random variables is the prob. distribution of  the variables contained in the subset.&lt;/p&gt;

&lt;h2 id=&#34;junction-trees&#34;&gt;Junction Trees&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://www.cs.helsinki.fi/u/bmmalone/probabilistic-models-spring-2014/JunctionTreeBarber.pdf&#34;&gt;David Barber&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;</description>
    </item>
    
    <item>
      <title>ML Notes on Learning and working with DNN</title>
      <link>http://s-aguinaga.github.io/post/learning-and-working-with-deep-nn/</link>
      <pubDate>Sun, 13 Nov 2016 06:53:43 -0500</pubDate>
      
      <guid>http://s-aguinaga.github.io/post/learning-and-working-with-deep-nn/</guid>
      <description>&lt;p&gt;Deep learning: A brief walk into the foundations underlying neural networks and deep learning.&lt;/p&gt;

&lt;p&gt;
&lt;!--.----1----.----2----.----3----.----4----.----5----.----6----.----7----.----8 --&gt;&lt;/p&gt;

&lt;h1 id=&#34;neural-networks&#34;&gt;Neural Networks&lt;/h1&gt;

&lt;p&gt;Representation learning is a set of methods that allows an algorithm automatically
find the Representations needed for detection or classification.&lt;/p&gt;

&lt;h2 id=&#34;historical-notes&#34;&gt;Historical Notes&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Convolutional Networks
These network architectures came from a power model architecture introduced by the neocognitron (Fukushima, 1980) for processing images and inspired by the structure of the mammalian visual system see LeCun et al. (1998).&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Rectified linear unit&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Models of symbolic reasoning&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Connectionism and concepts of distributed representation&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Back-propagtion is an algorithm that dominates the way we train deep models&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Sequence modeling tasks
An example is natural language processing tasks (used in Google technology)&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Kernel Machines&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Graphical models&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Deep belief neural network (Hinton 2006)&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;machine-learning&#34;&gt;Machine Learning&lt;/h2&gt;

&lt;p&gt;In machine learning we start out with a concrete problem or challenge. This problem is usually coupled to a dataset. You have arrived to a new level of computing when you are tackling problems at scale and in the core realm of BigData. In other words, now you have to be in tune with your computing resources (e.g, one needs to understand what CPUs and what GPUs are on target computing system).&lt;/p&gt;

&lt;p&gt;Types of learning
- Representation learning
- Deep learning
- Supervised, unsupervised, and semi-supervised learning&lt;/p&gt;

&lt;p&gt;Before we look at the various methods available in machine learning for solving problems,
let&amp;rsquo;s start by considering data and the questions around that data motivating our interest.&lt;/p&gt;

&lt;h2 id=&#34;the-data&#34;&gt;The Data&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;Training data&lt;/li&gt;
&lt;li&gt;Unseen data (or test data)&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;supervised-learning&#34;&gt;Supervised learning&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Support Vector Machine
This algorithm can handle and infinite number of features (or attributes)
SVM is a classification algorithm - the type of question you can ask is
if something belongs  to a particular class. The objective of this algorithm is to find the optimal separating hyperplane. Think in terms of what is the largest margin we can find on each side of the the line for the given training data.&lt;/p&gt;

&lt;p&gt;Is the data linearly separable, if not, then is do we need nonlinear separation?&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Links
&lt;a href=&#34;http://www.cs.columbia.edu/~kathy/cs4701/documents/jason_svm_tutorial.pdf&#34;&gt;Kathy at Columbia&lt;/a&gt;
&lt;a href=&#34;https://www.analyticsvidhya.com/blog/2015/10/understaing-support-vector-machine-example-code/&#34;&gt;analyticsvidhya&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Unsupervised learning&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&#34;minibatch-stochastic-gradient-descent-algorithm&#34;&gt;Minibatch stochastic gradient descent algorithm&lt;/h3&gt;

&lt;h3 id=&#34;objective-function&#34;&gt;Objective function&lt;/h3&gt;

&lt;ul&gt;
&lt;li&gt;Sum of differentiable functions (finding its minimums or maximums by iteration.&lt;/li&gt;

&lt;li&gt;&lt;p&gt;A differentiable function, from &lt;a href=&#34;https://en.wikipedia.org/wiki/Differentiable_function&#34;&gt;Wikipedia&lt;/a&gt;, In calculus (a branch of mathematics), a differentiable function of one real variable is a function whose derivative exists at each point in its domain.&lt;/p&gt;

&lt;h3 id=&#34;gradient-descent&#34;&gt;Gradient Descent&lt;/h3&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Stochastic gradient descent&lt;/p&gt;

&lt;h2 id=&#34;system-setup&#34;&gt;System Setup&lt;/h2&gt;

&lt;p&gt;Setting up a dev machine.
- Note that on a MacBook Pro (Retina, 13-inch, Early 2015) Theano cannot use the GPU devices.
- On a mid 2010 mac mini with NVIDIA graphics&lt;/p&gt;

&lt;h3 id=&#34;mac-mini-with-nvidia-graphics&#34;&gt;Mac Mini with NVIDIA graphics&lt;/h3&gt;

&lt;p&gt;Getting the code&lt;a href=&#34;https://developer.nvidia.com/cuda-downloads&#34;&gt; CUDA downloads&lt;/a&gt;
- Specifically getting this image: &lt;a href=&#34;https://developer.nvidia.com/compute/cuda/8.0/Prod/local_installers/cuda_8.0.55_mac-dmg&#34;&gt;CUDA Toolkit dmg&lt;/a&gt;&lt;/p&gt;

&lt;h3 id=&#34;working-locally&#34;&gt;Working Locally&lt;/h3&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;&lt;a href=&#34;https://amaral.northwestern.edu/resources/guides/mounting-remote-folder-os-x-over-ssh&#34;&gt;Mounting remote folder on OS X over SSH&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;TensorFlow working on a Mac OS X(sierra) with Jupyter and Python3&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;code&gt;brew install python3&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Warning: Building python3 from source:
The bottle needs the Apple Command Line Tools to be installed.
You can install them, if desired, with: &lt;code&gt;xcode-select --install&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;a href=&#34;http://joebergantine.com/blog/2015/apr/30/installing-python-2-and-python-3-alongside-each-ot/&#34;&gt;http://joebergantine.com/blog/2015/apr/30/installing-python-2-and-python-3-alongside-each-ot/&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Getting jupyter working with py3 isn&amp;rsquo;t woking so I&amp;rsquo;ve had to research and follow
these instructions:
&lt;a href=&#34;http://ipython.readthedocs.io/en/stable/install/kernel_install.html&#34;&gt;http://ipython.readthedocs.io/en/stable/install/kernel_install.html&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;pre&gt;&lt;code&gt; # To activate this environment, use:
 # &amp;gt; source activate ipykernel_py3
 #
 # To deactivate this environment, use:
 # &amp;gt; source deactivate ipykernel_py3
 #
&lt;/code&gt;&lt;/pre&gt;

&lt;h4 id=&#34;working-locally-on-mac-os&#34;&gt;Working locally on Mac OS&lt;/h4&gt;

&lt;p&gt;To set it up here is what I had to do:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;saguinag@sailntrpy:~$ pip install --upgrade pip
Collecting pip
  Downloading pip-9.0.1-py2.py3-none-any.whl (1.3MB)
    100% |████████████████████████████████| 1.3MB 971kB/s
Installing collected packages: pip
  Found existing installation: pip 9.0.0
    Uninstalling pip-9.0.0:
      Successfully uninstalled pip-9.0.0
Successfully installed pip-9.0.1
saguinag@sailntrpy:~$ sudo -H pip install --upgrade virtualenv
Requirement already up-to-date: virtualenv in /usr/local/lib/python3.5/site-packages
saguinag@sailntrpy:~$ $ virtualenv --system-site-packages ~/tensorflow


~$ which virtualenv
	/usr/local/bin/virtualenv

virtualenv --system-site-packages ~/tensorflow
	Using base prefix &#39;/usr/local/Cellar/python3/3.5.2_3/Frameworks/Python.framework/Versions/3.5&#39;
	New python executable in /Users/saguinag/tensorflow/bin/python3.5
	Not overwriting existing python script /Users/saguinag/tensorflow/bin/python (you must use /Users/saguinag/tensorflow/bin/python3.5)
	Installing setuptools, pip, wheel...done.

source ~/tensorflow/bin/activate
(tensorflow) saguinag@sailntrpy:~$
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;connect-to-dsgx&#34;&gt;Connect to DSGx&lt;/h3&gt;

&lt;p&gt;If it applies, first connect via VPN. Using the following
command mount DSGx locally (on Mac OS X):
  &lt;code&gt;sshfs -o IdentityFile=~/.ssh/id_rsa username@dsg2.crc.nd.edu:/home/username/ /local/Vol/working_dir/&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;But this doesn&amp;rsquo;t work with virtualenv, so login via ssh to dsgx (!! this might not be entirely true.)&lt;/p&gt;

&lt;p&gt;To umount do:
    &lt;code&gt;umount -f DIR/&lt;/code&gt;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;http://docs.python-guide.org/en/latest/dev/virtualenvs/&#34;&gt;To work with virtualenv &lt;code&gt;ssh dsgx&lt;/code&gt;&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;code&gt;virtualenv -p /usr/bin/python2.7 venv/&lt;/code&gt;
&lt;code&gt;source venv/bin/activate&lt;/code&gt;&lt;/p&gt;

&lt;h4 id=&#34;with-dsg2-and-tensorflow-using-virtualenv&#34;&gt;With DSG2 and Tensorflow using virtualenv&lt;/h4&gt;

&lt;p&gt;Got it to work on DSG2:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;  513  virtualenv --system-site-packages ~/tensorflow
  514  source ~/tensorflow/bin/activate
  515  export TF_BINARY_URL=https://storage.googleapis.com/tensorflow/linux/cpu/tensorflow-0.11.0rc2-cp27-none-linux_x86_64.whl
  516  pip install --upgrade $TF_BINARY_URL
  517  python
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;tensorflow&#34;&gt;TensorFlow&lt;/h2&gt;

&lt;p&gt;Links to &lt;a href=&#34;https://www.tensorflow.org/versions/r0.11/tutorials/index.html&#34;&gt;tensorflow tutorials&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;ToDo: Need to install it locally for now&lt;/p&gt;

&lt;p&gt;The working directory is &lt;code&gt;/Volumes/theory/entropy/DeepLerning&lt;/code&gt;. Go into this directory
and launch Jupyter using the following command, &lt;code&gt;jupyter notebook&amp;amp;&lt;/code&gt;&lt;/p&gt;

&lt;h3 id=&#34;basic-test&#34;&gt;Basic Test&lt;/h3&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/tensorflow/tensorflow&#34;&gt;TensorFlow Site&lt;/a&gt; Done&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/tensorflow/tensorflow/blob/master/tensorflow/g3doc/get_started/os_setup.md#test-the-tensorflow-installation&#34;&gt;Run a TensorFlow demo model&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;theano&#34;&gt;Theano&lt;/h2&gt;

&lt;p&gt;On your favorite Search Engine type &lt;code&gt;download install theano&lt;/code&gt; and you will end up this site: the docs for Theano 0.8 can be found &lt;a href=&#34;http://deeplearning.net/software/theano/install.html&#34;&gt;Installing Theano Docs&lt;/a&gt; and now depending on your platform got to the specific instructions and &lt;code&gt;pip install&lt;/code&gt; it.&lt;/p&gt;

&lt;p&gt;Following are my development notes.&lt;/p&gt;

&lt;h3 id=&#34;to-test-that-the-system-has-gpu-capability&#34;&gt;To test that the system has GPU capability&lt;/h3&gt;

&lt;p&gt;My MacBookPro&amp;rsquo;s hardware Intel Iris Graphics 6100 is &lt;a href=&#34;http://stackoverflow.com/questions/32045279/how-can-i-enable-my-macbook-pro-gpu-optimization-for-theano&#34;&gt;apparently not compatible with Theano in GPU mode&lt;/a&gt;. Thus, we are going to switch to a hardware system with NVIDIA graphics.&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;http://deeplearning.net/software/theano_versions/dev/install_macos.html&#34;&gt;Installation macOS&lt;/a&gt;&lt;/p&gt;

&lt;h3 id=&#34;examples&#34;&gt;Examples&lt;/h3&gt;

&lt;p&gt;&lt;a href=&#34;../../pages/BabyTheano.html&#34;&gt;Baby Steps&lt;/a&gt;&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>4th Heidelberg Laureate Forum</title>
      <link>http://s-aguinaga.github.io/post/hlf/</link>
      <pubDate>Mon, 26 Sep 2016 13:00:35 -0400</pubDate>
      
      <guid>http://s-aguinaga.github.io/post/hlf/</guid>
      <description>&lt;p&gt;I received and invitation and funding to attend the &lt;a href=&#34;http://www.heidelberg-laureate-forum.org&#34;&gt;4th Heildelberg Laureate Forum&lt;/a&gt;.&lt;/p&gt;

&lt;!-- There I got to meet and talk Nobel Laureates and Turin Award winners and an incredible group of young researchers from across the world.&lt;br&gt;&#34; --&gt;

&lt;p&gt;&lt;/p&gt;

&lt;p&gt;I had an extraordinary opportunity to participate as a young researcher in
this year&amp;rsquo;s 4th Heidelberg Laureate Forum (Heidelberg, Germany). I came away
having talked one-on-one with several Turin Award Winners and one Nobel Laureate&lt;/p&gt;

&lt;!-- ## Discovering Hidden Structure --&gt;</description>
    </item>
    
    <item>
      <title>Welcome</title>
      <link>http://s-aguinaga.github.io/post/welcome/</link>
      <pubDate>Sun, 28 Aug 2016 07:07:23 -0400</pubDate>
      
      <guid>http://s-aguinaga.github.io/post/welcome/</guid>
      <description>&lt;p&gt;The purpose of this Web-site is to tell you about my research interests, the projects
I am involved in, the things I make, and a little about who I am.
&lt;/br&gt;
Connect with me via:
&lt;a href=&#34;www.linkedin.com/in/sauginag&#34;&gt;LinkedIn&lt;/a&gt; | &lt;a href=&#34;https://twitter.com/abitofalchemy&#34;&gt;Twitter&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;/p&gt;

&lt;h2 id=&#34;extended-bio&#34;&gt;Extended Bio&lt;/h2&gt;

&lt;p&gt;Professional experience includes stints in auditory research and electrophysiology at Northwestern University,
a hardware design at Motorola (mobile devices) and 3Com (now, HPE Networking).&lt;/p&gt;

&lt;h2 id=&#34;credits&#34;&gt;Credits&lt;/h2&gt;

&lt;p&gt;The site is an evolving project. I built it with Hugo because I can write content using
markdown.&lt;/p&gt;</description>
    </item>
    
  </channel>
</rss>